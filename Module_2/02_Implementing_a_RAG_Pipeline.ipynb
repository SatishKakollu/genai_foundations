{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bd6aaee-4113-4edc-9c1f-f5364d0e8ba5",
   "metadata": {},
   "source": [
    "## Module 2, Activity 2: Implementing a RAG Pipeline\n",
    "\n",
    "Up until this point we have created an in-memory vector database and populated it with some text.  Now we are going to see how we can create a pipeline with this database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdc6aef9-9060-4c49-af94-626b3b63cd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "from langchain_aws import BedrockEmbeddings, ChatBedrock, ChatBedrockConverse\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cf2157a-5ccd-4d68-8d0e-4a7800c2a0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_s3(bucket_name, key):\n",
    "    s3 = boto3.client(\n",
    "        's3',\n",
    "        region_name=\"us-west-2\",\n",
    "    )\n",
    "    response = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "    data = response['Body'].read().decode('utf-8')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb508d28-9038-4ced-b942-f5a642e90122",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\", region_name='us-west-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062cd158-05d2-4978-8e75-cb897e2d977f",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "As before, we are going to follow the same steps for creating our vectors for RAG:\n",
    "\n",
    "1. Load in text data\n",
    "2. Create a splitter with a defined chunking strategy\n",
    "3. Create chunks (a list of LangChain documents)\n",
    "4. Create an embedding for each chunk\n",
    "5. Store the chunk and embedding in the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9edc1939-d80a-436c-9b54-2e1d5bd45b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BILL Reports Second Quarter Fiscal Year 2025 Financial Results\\nFebruary 6, 2025\\n\\n\\t•\\tQ2 Core Revenue Increased 16% Year-Over-Year\\n\\t•\\tQ2 Total Revenue Increased 14% Year-Over-Year\\nSAN JOSE, Calif.--(BUS'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_data = get_data_from_s3(\"dpgenaitraining\", \"q2_results.txt\")\n",
    "s3_data[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86889026-35f8-40f8-9479-051bd44d6717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 43\n"
     ]
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "chunks = splitter.create_documents([s3_data])\n",
    "print(f\"Total chunks created: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5932c6ad-efc5-4bcc-899c-676bc7ab957e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v2:0\", client=bedrock_runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65a4ab07-8cee-48f9-b372-f562f555b51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = InMemoryVectorStore.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d5b345-335f-46a2-bce6-fd91e2a142fc",
   "metadata": {},
   "source": [
    "## Bringing the LLM back into it\n",
    "\n",
    "In Module 3, Activity 1 we performed a cosine similarity to find the top `k` most similar documents.  However, that is not really a useful way to look at things based on the fact that what is returned reads like a chunk of arbitrary text.  It would be much better if those chunks were turned into a easily-understood text.  And so now we will bring the LLM back in!\n",
    "\n",
    "We will use an LLM (note that it is a different LLM than used for the embeddings!) to take the returned information from the vector store and turn it into an actual answer to the question.  In order to do so, we have to _retrieve_ information from the vector store first and we then run those retrieved results through the LLM.  This should remind you of the chains we created in previous modules, and it should!  We actually will create a retrieval question-answer chain using `RetrievalQA` below, where we chain our LLM and retriever together.  (Note that you could -- and should! -- add a prompt to this, but we are leaving it out now for the save of breavity.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1fc473c-71ba-485b-acad-cd621dca2ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatBedrockConverse(\n",
    "    model=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da8807ae-70eb-4284-bb60-ea7fa083816c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What were the subscription fees?',\n",
       " 'result': 'According to the financial highlights provided, the subscription fees for the second quarter were $67.7 million, up 7% year-over-year.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain.invoke('What were the subscription fees?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa0f995-4f34-4415-b15c-425b654ac62c",
   "metadata": {},
   "source": [
    "## Concluding thoughts\n",
    "\n",
    "We now have all of the things we need to create RAG pipelines.  Be sure to experiment with the question you ask.  Try different chunking strategies to see what happens to the output.  Try adding in a prompt.  Most importantly, create a different chain that does NOT have the retriever in it to see what kind of different answers you get when you don't use RAG.  \n",
    "\n",
    "Hopefully you see that using RAG is the key to minimizing hallucinations and making your GenAI applications as relevant as possible!\n",
    "\n",
    "**Remember:** If you are using RAG (i.e. you are creating embeddings), you will want to create your embeddings of text.  Depending on what you are creating vectors of, this means you might need to convert your data source (such as tables) into text first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b53593-1e14-4374-a740-54774a544730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
