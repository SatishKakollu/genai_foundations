{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "203c8da7-d7a5-43fe-a5da-dfcf0ae107d5",
   "metadata": {},
   "source": [
    "# Module 1, Activity 3: Evaluating and Refining Model Outputs Using LLM-Based Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "645adc0e-20a8-4bc9-be54-221e3829144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1da65187-071e-4947-8e96-ad1154484140",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\", region_name='us-west-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94ba588-942e-4735-9bfa-f907ad233f5e",
   "metadata": {},
   "source": [
    "## The Importance of Evaluation in your GenAI Solutions\n",
    "\n",
    "You can write a really great app, conduct a lot of prompt engineering work, and even fine tune an LLM to your use case.  But all of this is wasted effort without being able to state whether your work is actually improving anything.\n",
    "\n",
    "Many of the best minds will tell you that the very first thing you need to do in developing an GenAI solution is determine how it will be evaluated.  What is more fun is that you can actually use an LLM to evaluate the results of an LLM!\n",
    "In fact, as your evaluation datasets get larger, you will likely find that this is one of the only viable ways to test.\n",
    "\n",
    "There are many ways to evaluate your LLM (LangChain even has their own, called LangSmith).  Ultimately, the gold standard comes down to providing a series of question-answer pairs that the model is evaluated against and providing the prompt with a rubric for evaluating the returned results to those QA pairs.  The wording of your prompt is very important to creating a good, automated evaluator.  So I encourage you to run the rest of the code in this notebook, experimenting with this prompt to make your evaluations more and more accurate based on prompt wording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f03608d-f50b-4e28-9439-f428570b5016",
   "metadata": {},
   "outputs": [],
   "source": [
    "grading_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a grader evaluating whether an AI-generated answer is correct and contextually accurate compared to a reference answer.\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "REFERENCE ANSWER:\n",
    "{reference_answer}\n",
    "\n",
    "AI-GENERATED ANSWER:\n",
    "{generated_answer}\n",
    "\n",
    "Score the AI answer from 1 (completely wrong) to 5 (perfectly correct and well-reasoned).\n",
    "Then briefly justify your score.\n",
    "\n",
    "Respond in this format:\n",
    "Score: <number>\n",
    "Justification: <your explanation>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0c22f2-61f4-408e-b61d-f00aeacd4141",
   "metadata": {},
   "source": [
    "## Generate and Evaluate AI Answer\n",
    "\n",
    "This function generates an LLM response to a given question (done with the `generator_llm`) and the compares the output to the `reference_answer` for scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e25eee5-287d-4cf4-9e93-f4d670508627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_generated_answer(question: str, reference_answer: str, generated_answer: str) -> dict:\n",
    "    messages = grading_prompt.format_messages(\n",
    "        question=question,\n",
    "        reference_answer=reference_answer,\n",
    "        generated_answer=generated_answer\n",
    "    )\n",
    "    response = grader_llm(messages)\n",
    "    content = response.content.strip()\n",
    "\n",
    "    lines = content.split(\"\\n\")\n",
    "    score_line = next((l for l in lines if l.lower().startswith(\"score\")), \"Score: 0\")\n",
    "    justification_line = next((l for l in lines if l.lower().startswith(\"justification\")), \"Justification: N/A\")\n",
    "\n",
    "    return {\n",
    "        \"score\": int(score_line.split(\":\")[1].strip()),\n",
    "        \"justification\": justification_line.split(\":\", 1)[1].strip(),\n",
    "        \"generated_answer\": generated_answer\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90acf8e7-9227-4b1c-8442-f6d723c97817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question: str) -> str:\n",
    "    return generator_llm.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61f412f-eccf-4a61-a506-876e98eeae68",
   "metadata": {},
   "source": [
    "## On the Creation of QA Pairs\n",
    "\n",
    "Ultimately, you will want to create a nice, long list of QA pairs that are customized to your given application.  Be sure to get creative.  For example, the list below starts off with some very easy, knowable questions.  But then it gets a bit more subjective, even going down the direction of opinions.  \n",
    "\n",
    "Spend some time thinking long and hard how to develop QA pairs for your application.  Test edge cases.  Be sure, particularly for agentic workflows, that each tool is being tests thoroughly.  And do notice that there might be a question or two below that either represent an opinion or are factually incorrect.  Watch what happens when you run these through your test.  How might you address that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46faaa11-fcce-4437-b6a2-fec72adad7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = [\n",
    "    {\n",
    "        \"question\": \"What is the capital of France?\",\n",
    "        \"reference_answer\": \"Paris is the capital of France. It's located in the northern part of the country.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is 5 + 7?\",\n",
    "        \"reference_answer\": \"First, we add 5 and 7. The result is 12.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Why does the sun rise in the east?\",\n",
    "        \"reference_answer\": \"Because the Earth rotates west to east, the sun appears to rise in the east.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the answer to life, the universe, and everything?\",\n",
    "        \"reference_answer\": \"42 is the answer to life, the universe, and everything.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Does pineapple belong on a pizza?\",\n",
    "        \"reference_answer\": \"Yes, absolutely.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the best operating system?\",\n",
    "        \"reference_answer\": \"Linux\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the largest planet in the solar system?\",\n",
    "        \"reference_answer\": \"Mercury\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5abf54a-6cba-4472-9c6f-efe0a74dd49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_llm = ChatBedrockConverse(\n",
    "    model=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "grader_llm = ChatBedrockConverse(\n",
    "    model=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d4c253-c47e-461e-9fe8-d26c92a03edf",
   "metadata": {},
   "source": [
    "Let's now run the graders against all of the questions.  Be sure to try running this a few different times since LLMs are probabilistic and generate different results every time, regardless of how you set their hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06a112b8-fa3b-435e-89cd-eed56373547a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- QA #1 ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'generate_answer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, qa \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(qa_pairs, \u001b[32m1\u001b[39m):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- QA #\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     gen_answer = \u001b[43mgenerate_answer\u001b[49m(qa[\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      4\u001b[39m     result = grade_generated_answer(\n\u001b[32m      5\u001b[39m         question=qa[\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      6\u001b[39m         reference_answer=qa[\u001b[33m\"\u001b[39m\u001b[33mreference_answer\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      7\u001b[39m         generated_answer=gen_answer\n\u001b[32m      8\u001b[39m     )\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mQuestion:\u001b[39m\u001b[33m\"\u001b[39m, qa[\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'generate_answer' is not defined"
     ]
    }
   ],
   "source": [
    "for i, qa in enumerate(qa_pairs, 1):\n",
    "    print(f\"\\n--- QA #{i} ---\")\n",
    "    gen_answer = generate_answer(qa[\"question\"])\n",
    "    result = grade_generated_answer(\n",
    "        question=qa[\"question\"],\n",
    "        reference_answer=qa[\"reference_answer\"],\n",
    "        generated_answer=gen_answer\n",
    "    )\n",
    "\n",
    "    print(\"Question:\", qa[\"question\"])\n",
    "    print(\"Generated Answer:\", result[\"generated_answer\"])\n",
    "    print(\"Score:\", result[\"score\"])\n",
    "    print(\"Justification:\", result[\"justification\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466f776b-8ffb-49a1-b1fc-4c3be2a05b52",
   "metadata": {},
   "source": [
    "## Concluding Thoughts\n",
    "\n",
    "There are many different metrics that an LLM-based application can be evaluated on.  [This documentation](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators) by LangChain shows some of the off-the-shelf evaluators built into LangSmith and is a great starting point.  You can also develop your own.\n",
    "\n",
    "As you prepare to deploy your own GenAI apps to production, think about regularly scheduling tests for your apps to make sure that you are not drifting based on whatever your chosen metrics are.  Creating your testing strategy up front will save you a lot of time down the road!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c9f707-f1b6-43c8-9f88-5a03072de171",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
