{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3544afc1-dbb9-4af8-a4d5-db51feec40d5",
   "metadata": {},
   "source": [
    "# Module 1, Activity 1: Getting Started with Bedrock and Basic Prompting\n",
    "\n",
    "In this notebook we are going to demonstrate how to use the `boto3` Python SDK along with a variety of abstractions available through the LangChain package to work with the Amazon Bedrock Foundational Models.  By the end of this notebook you will be able to create a basic chain with a simple prompt capable of asking questions to the model and outputing the answer in a human-readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00bef1a8-6316-4682-8f36-7a8c3730ae58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "from langchain_aws import BedrockLLM, ChatBedrock\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd4f71ea-3a65-4e35-9fcc-40f6583ae1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.session.Session()\n",
    "region = session.region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05caca4a-483c-47cd-8936-1d0d0097bc92",
   "metadata": {},
   "source": [
    "## Create Bedrock management connection\n",
    "\n",
    "The Bedrock client is used as the control for Bedrock.  It can do things like list models, check availability, and manage configurations.  But it doesn't actually do anything with the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f60658b-5181-4122-80d7-9e1787ba421a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Models: ['amazon.titan-tg1-large', 'amazon.nova-premier-v1:0:8k', 'amazon.nova-premier-v1:0:20k', 'amazon.nova-premier-v1:0:1000k', 'amazon.nova-premier-v1:0:mm', 'amazon.nova-premier-v1:0', 'amazon.titan-embed-g1-text-02', 'amazon.titan-text-lite-v1:0:4k', 'amazon.titan-text-lite-v1', 'amazon.titan-text-express-v1:0:8k', 'amazon.titan-text-express-v1', 'amazon.nova-pro-v1:0', 'amazon.nova-lite-v1:0', 'amazon.nova-micro-v1:0', 'amazon.titan-embed-text-v1:2:8k', 'amazon.titan-embed-text-v1', 'amazon.titan-embed-text-v2:0', 'amazon.titan-embed-image-v1:0', 'amazon.titan-embed-image-v1', 'amazon.titan-image-generator-v1:0', 'amazon.titan-image-generator-v1', 'amazon.titan-image-generator-v2:0', 'amazon.rerank-v1:0', 'stability.stable-diffusion-xl-v1:0', 'stability.stable-diffusion-xl-v1', 'stability.sd3-large-v1:0', 'stability.sd3-5-large-v1:0', 'stability.stable-image-core-v1:0', 'stability.stable-image-core-v1:1', 'stability.stable-image-ultra-v1:0', 'stability.stable-image-ultra-v1:1', 'anthropic.claude-3-5-sonnet-20241022-v2:0:18k', 'anthropic.claude-3-5-sonnet-20241022-v2:0:51k', 'anthropic.claude-3-5-sonnet-20241022-v2:0:200k', 'anthropic.claude-3-5-sonnet-20241022-v2:0', 'anthropic.claude-3-7-sonnet-20250219-v1:0', 'anthropic.claude-3-5-haiku-20241022-v1:0', 'anthropic.claude-instant-v1:2:100k', 'anthropic.claude-instant-v1', 'anthropic.claude-v2:0:18k', 'anthropic.claude-v2:0:100k', 'anthropic.claude-v2:1:18k', 'anthropic.claude-v2:1:200k', 'anthropic.claude-v2:1', 'anthropic.claude-v2', 'anthropic.claude-3-sonnet-20240229-v1:0:28k', 'anthropic.claude-3-sonnet-20240229-v1:0:200k', 'anthropic.claude-3-sonnet-20240229-v1:0', 'anthropic.claude-3-haiku-20240307-v1:0:48k', 'anthropic.claude-3-haiku-20240307-v1:0:200k', 'anthropic.claude-3-haiku-20240307-v1:0', 'anthropic.claude-3-opus-20240229-v1:0:12k', 'anthropic.claude-3-opus-20240229-v1:0:28k', 'anthropic.claude-3-opus-20240229-v1:0:200k', 'anthropic.claude-3-opus-20240229-v1:0', 'anthropic.claude-3-5-sonnet-20240620-v1:0:18k', 'anthropic.claude-3-5-sonnet-20240620-v1:0:51k', 'anthropic.claude-3-5-sonnet-20240620-v1:0:200k', 'anthropic.claude-3-5-sonnet-20240620-v1:0', 'cohere.command-text-v14:7:4k', 'cohere.command-text-v14', 'cohere.command-r-v1:0', 'cohere.command-r-plus-v1:0', 'cohere.command-light-text-v14:7:4k', 'cohere.command-light-text-v14', 'cohere.embed-english-v3:0:512', 'cohere.embed-english-v3', 'cohere.embed-multilingual-v3:0:512', 'cohere.embed-multilingual-v3', 'cohere.rerank-v3-5:0', 'deepseek.r1-v1:0', 'meta.llama3-8b-instruct-v1:0', 'meta.llama3-70b-instruct-v1:0', 'meta.llama3-1-8b-instruct-v1:0:128k', 'meta.llama3-1-8b-instruct-v1:0', 'meta.llama3-1-70b-instruct-v1:0:128k', 'meta.llama3-1-70b-instruct-v1:0', 'meta.llama3-1-405b-instruct-v1:0', 'meta.llama3-2-11b-instruct-v1:0:128k', 'meta.llama3-2-11b-instruct-v1:0', 'meta.llama3-2-90b-instruct-v1:0:128k', 'meta.llama3-2-90b-instruct-v1:0', 'meta.llama3-2-1b-instruct-v1:0:128k', 'meta.llama3-2-1b-instruct-v1:0', 'meta.llama3-2-3b-instruct-v1:0:128k', 'meta.llama3-2-3b-instruct-v1:0', 'meta.llama3-3-70b-instruct-v1:0', 'meta.llama4-scout-17b-instruct-v1:0', 'meta.llama4-maverick-17b-instruct-v1:0', 'mistral.mistral-7b-instruct-v0:2', 'mistral.mixtral-8x7b-instruct-v0:1', 'mistral.mistral-large-2402-v1:0', 'mistral.mistral-large-2407-v1:0', 'mistral.pixtral-large-2502-v1:0', 'luma.ray-v2:0', 'writer.palmyra-x4-v1:0', 'writer.palmyra-x5-v1:0']\n"
     ]
    }
   ],
   "source": [
    "bedrock = boto3.client(\"bedrock\", region_name='us-west-2')\n",
    "model_ids = [model[\"modelId\"] for model in bedrock.list_foundation_models()[\"modelSummaries\"]]\n",
    "print(\"Available Models:\", model_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6112bdf3-c9a1-48e7-8e76-a9140ac166b3",
   "metadata": {},
   "source": [
    "## Initializing BedrockLLM and invoking a model\n",
    "\n",
    "Here, the BedrockLLM class from the langchain_aws package is instantiated.  This class serves as a high-level wrapper to interface with AWS-hosted LLMs.\n",
    "The initialization parameters include the model ID (in this case, \"amazon.titan-tg1-large\"), region, and the necessary AWS credentials.  Once the instance is created, the invoke method is used to send a prompt (\"What is the recipe of mayonnaise?\") to the model.  This section demonstrates the fundamental workflow: setting up the model wrapper and making a basic invocation call to test the modelâ€™s response, providing a concrete example of how to interact with AWS-hosted generative AI models using LangChain.\n",
    "\n",
    "Try several different prompts here to see what different types of answers you can get!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fad2188-a75c-47f3-894f-c7793dfbd492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here is the recipe for mayonnaise:\n",
      "\n",
      "1. 1 cup of vegetable oil\n",
      "2. 2 egg yolks\n",
      "3. 1 tablespoon of lemon juice or vinegar\n",
      "4. 1 teaspoon of mustard powder\n",
      "5. 1 teaspoon of salt\n",
      "\n",
      "1. Whisk together the egg yolks, lemon juice, mustard powder, and salt in a bowl.\n",
      "2. Gradually add the vegetable oil, whisking continuously until the mixture thickens.\n",
      "3. Once thickened, transfer the mayonnaise to a jar and refrigerate until ready to use.\n"
     ]
    }
   ],
   "source": [
    "simple_llm = BedrockLLM(\n",
    "    model_id=\"amazon.titan-tg1-large\",\n",
    "    region_name='us-west-2',\n",
    ")\n",
    "print(simple_llm.invoke(\"What is the recipe for mayonnaise?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a440b8f-c213-40f2-a727-8d534d1b4ddd",
   "metadata": {},
   "source": [
    "## Introducing ChatBedrock\n",
    "\n",
    "BedrockLLM is designed for single-turn, prompt-based interactions where you provide the prompt (\"What is the recipe for mayonnaise?\") and the model generates an output in one go.  This is fine for simple things, but when you need to have more sophisticated interactions you want something that supports chat-like exchanges where the model can manage context over several turns of dialogue.  Additionally, not all of the available models, including more sophisticated models like Anthropic's Claude 3 Sonnet below, are supported by BedrockLLM.  Hence, we have the more sophisticated ChatBedrock, as shown below.\n",
    "\n",
    "Also note that the output of ChatBedrock contains much more information than just a text output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2781d5c1-b30b-43c4-a161-d4d9b97423ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Here is a basic recipe for making mayonnaise from scratch:\\n\\nIngredients:\\n- 1 egg yolk\\n- 1 tablespoon lemon juice or white wine vinegar\\n- 1/2 teaspoon dijon mustard (optional, for flavor)\\n- 1/4 teaspoon salt\\n- 3/4 cup neutral oil like canola, vegetable or grapeseed oil\\n\\nInstructions:\\n\\n1. In a medium bowl, whisk together the egg yolk, lemon juice/vinegar, mustard (if using) and salt.\\n\\n2. Very slowly, while whisking constantly, drizzle in a few drops of the oil until the mixture begins to thicken and emulsify.\\n\\n3. Still whisking constantly, start adding the oil in a thin steady stream, going slowly to allow the mixture to emulsify and thicken.\\n\\n4. Once all the oil has been incorporated and the mayonnaise is thick, you can adjust seasoning if needed by adding more lemon juice, salt, etc.\\n\\n5. Transfer to an airtight container and refrigerate for up to 5 days.\\n\\nThe keys are using a room temperature egg yolk, slowly drizzling in the oil while whisking vigorously to allow a stable emulsion, and whisking constantly throughout adding the oil. Taking your time is important for proper emulsification.' additional_kwargs={'usage': {'prompt_tokens': 16, 'completion_tokens': 308, 'total_tokens': 324}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'} response_metadata={'usage': {'prompt_tokens': 16, 'completion_tokens': 308, 'total_tokens': 324}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'} id='run-5013451c-a0ac-4a9a-9993-20410e30d94a-0' usage_metadata={'input_tokens': 16, 'output_tokens': 308, 'total_tokens': 324}\n"
     ]
    }
   ],
   "source": [
    "chat_llm = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    region_name='us-west-2',\n",
    ")\n",
    "print(chat_llm.invoke(\"What is the recipe for mayonnaise?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d1cbbf-3211-4c86-b97e-53eba24a747e",
   "metadata": {},
   "source": [
    "## Temperature\n",
    "\n",
    "Temperature is the thing that gives models creativity.  It controls the randomness of the model's responses.  Setting it to 0.0 (the minimum) typically results in a more deterministic and consistent output while setting it to 1.0 (the maximum) results in more creative responses.  \n",
    "\n",
    "- Low temperature (e.g., 0.0-0.3): The model plays it safe, sticking to the most likely answers. This is great when you want accuracy and consistency, like coding help or fact-based answers.\n",
    "\n",
    "- High temperature (e.g., 0.7-1.0): The model gets more adventurous, picking less common words and generating more diverse responses. This is useful for creative writing, brainstorming, or when you want unique outputs.\n",
    "\n",
    "If you set it to 0, the model is basically deterministicâ€”itâ€™ll always give the same answer if asked the same thing.  As you increase temperature you will get more creative (and perhaps unpredictable!) responses.  So let's create a simple prompt and run it a few times.  Experiment with both the prompt and the temperature of the following cell and observe the effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a140a6f1-59d6-4290-bebc-f740697e41a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"To generate boilerplate code for a payment processing microservice in Python that integrates with a payment gateway API, we need to consider several key aspects, including security, error handling, and logging. Here's an explanation of how I would approach this task:\\n\\n1. **Security**:\\n   - **Token-based Authentication**: Implement token-based authentication to secure the communication between the microservice and the payment gateway API. This can be achieved using industry-standard protocols like OAuth 2.0 or JSON Web Tokens (JWT). The microservice should obtain an access token from the payment gateway API and include it in the headers of all subsequent requests.\\n   - **Encryption**: Ensure that sensitive data, such as credit card numbers and other payment details, are encrypted both in transit and at rest. Use secure protocols like HTTPS for communication and encrypt data stored in databases or files using strong encryption algorithms like AES-256.\\n\\n2. **Error Handling**:\\n   - Implement robust error handling mechanisms to gracefully handle exceptions and errors that may occur during the payment processing workflow.\\n   - Define custom exception classes for different types of errors (e.g., `PaymentGatewayError`, `InvalidPaymentDataError`, etc.) to make it easier to identify and handle specific error scenarios.\\n   - Provide clear and informative error messages to aid in debugging and troubleshooting.\\n\\n3. **Logging**:\\n   - Implement comprehensive logging to track the execution flow, capture errors, and record relevant information for auditing and debugging purposes.\\n   - Use a logging library like `logging` in Python to configure different log levels (e.g., `DEBUG`, `INFO`, `WARNING`, `ERROR`) and log handlers (e.g., file, console, remote logging service).\\n   - Log sensitive information, such as payment details or API keys, with caution or avoid logging them altogether to prevent data leaks.\\n\\nHere's a sample code snippet that demonstrates some of these concepts:\\n\\n```python\\nimport logging\\nimport requests\\nfrom requests.auth import HTTPBasicAuth\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\\n\\n# Payment Gateway API credentials\\nAPI_KEY = 'your_api_key'\\nAPI_SECRET = 'your_api_secret'\\n\\n# Payment Gateway API endpoint\\nPAYMENT_GATEWAY_URL = 'https://api.paymentgateway.com/v1/payments'\\n\\ndef process_payment(payment_data):\\n    try:\\n        # Obtain access token from Payment Gateway API\\n        auth = HTTPBasicAuth(API_KEY, API_SECRET)\\n        token_response = requests.post('https://api.paymentgateway.com/v1/token', auth=auth)\\n        token_response.raise_for_status()\\n        access_token = token_response.json()['access_token']\\n\\n        # Send payment request to Payment Gateway API\\n        headers = {'Authorization': f'Bearer {access_token}'}\\n        payment_response = requests.post(PAYMENT_GATEWAY_URL, json=payment_data, headers=headers)\\n        payment_response.raise_for_status()\\n\\n        # Process successful payment response\\n        logging.info('Payment successful: %s', payment_response.json())\\n        return payment_response.json()\\n\\n    except requests.exceptions.RequestException as e:\\n        logging.error('Payment Gateway API error: %s', e)\\n        raise PaymentGatewayError(str(e))\\n    except Exception as e:\\n        logging.error('Unexpected error: %s', e)\\n        raise\\n\\nclass PaymentGatewayError(Exception):\\n    pass\\n\\n# Example usage\\npayment_data = {\\n    'amount': 100.0,\\n    'currency': 'USD',\\n    'card_number': '4111111111111111',\\n    'expiry_month': '12',\\n    'expiry_year': '2025',\\n    'cvv': '123'\\n}\\n\\ntry:\\n    payment_result = process_payment(payment_data)\\n    # Handle successful payment\\nexcept PaymentGatewayError as e:\\n    # Handle Payment Gateway API errors\\n    logging.error('Payment Gateway error: %s', e)\\nexcept Exception as e:\\n    # Handle unexpected errors\\n    logging.error('Unexpected error: %s', e)\\n```\\n\\nIn this example, the `process_payment` function handles the payment processing workflow. It obtains an access token from the payment gateway API using basic\" additional_kwargs={'usage': {'prompt_tokens': 68, 'completion_tokens': 1024, 'total_tokens': 1092}, 'stop_reason': 'max_tokens', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'} response_metadata={'usage': {'prompt_tokens': 68, 'completion_tokens': 1024, 'total_tokens': 1092}, 'stop_reason': 'max_tokens', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'} id='run-dcc4dec3-0aa9-4b23-be06-66f0c66af687-0' usage_metadata={'input_tokens': 68, 'output_tokens': 1024, 'total_tokens': 1092}\n"
     ]
    }
   ],
   "source": [
    "chat_llm_temp = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    region_name='us-west-2',\n",
    "    temperature=0.0\n",
    ")\n",
    "print(chat_llm_temp.invoke(\"Write a short bedtime story about a robot.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3bff25-1d6c-4503-9fa3-4bdbaf456abf",
   "metadata": {},
   "source": [
    "## Limiting the number of tokens returned\n",
    "\n",
    "The cost of using an LLM is dependent on how many tokens are sent back and forth with the model.  The `max_tokens` parameter can provide a limit on how many total tokens are returned.  Limiting the token count can be useful when you need to ensure that the responses remain concise or when working within strict output size constraints.  Experiment with a few different values for this to see how the output changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee063ebc-b676-4c6f-9276-b78563a598fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Here is a basic recipe for homemade mayonnaise:\\n\\nIngredients:\\n- 1 egg yolk\\n- 1 tablespoon lemon juice or white wine vinegar\\n- 1/2 teaspoon dijon mustard (optional)\\n- 1/4 teaspoon salt\\n- 3/4 cup vegetable oil or mild olive oil\\n\\nInstructions:\\n\\n1. In a medium bowl, whisk together the egg yolk, lemon juice/vinegar, mustard (if using), and salt.\\n\\n2. Very slowly, while whisking constantly, drizzle in a few drops of the oil until the mixture begins to thicken. \\n\\n3. Once thickened, you can add the oil in a thin steady stream while continuing to whisk vigorously. \\n\\n4. Whisk until all the oil is incorporated and the mayonnaise is thick and emulsified.\\n\\n5. Taste and adjust seasoning if needed by adding more lemon juice, salt, etc.\\n\\n6. Transfer to an airtight container and refrigerate for up to 1 week.\\n\\nThe key is adding the oil very slowly while whisking constantly to allow a stable emulsion to form between the egg yolk and oil. Be patient and go slowly when adding the oil.' additional_kwargs={'usage': {'prompt_tokens': 16, 'completion_tokens': 287, 'total_tokens': 303}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'} response_metadata={'usage': {'prompt_tokens': 16, 'completion_tokens': 287, 'total_tokens': 303}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'} id='run-bf93dec6-f542-426a-b80f-54c91af1905f-0' usage_metadata={'input_tokens': 16, 'output_tokens': 287, 'total_tokens': 303}\n"
     ]
    }
   ],
   "source": [
    "chat_llm_limited = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    region_name='us-west-2',\n",
    "    temperature=0.0,\n",
    "    max_tokens=1000\n",
    ")\n",
    "print(chat_llm_limited.invoke(\"What is the recipe for mayonnaise?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3593fba2-8ed9-4b83-bf17-3ef8cf5368a3",
   "metadata": {},
   "source": [
    "## Creating a Chain with Prompts and Output Parsing\n",
    "\n",
    "Now that we have seen some of the basics, it is time to create an actual question-answering bot.  In order to turn this into a fully-functioning question-answering system, we need to combine the following three concepts:\n",
    "\n",
    "### Prompts and Prompt Templates\n",
    "\n",
    "In LangChain, **system prompts** (provide the foundational instructions and context for the AI model.  They define the model's role, behavior, and any specific output format or constraints, ensuring that all subsequent responses align with the intended purpose.  In contrast, **human prompts** represent the dynamic, user-supplied inputs that drive the conversation or query.  They capture the specific question or instruction that the user wants the model to address.  The `ChatPromptTemplate` acts as a framework that seamlessly integrates both types of prompts into a coherent message sequence.  By combining system and human messages, the `ChatPromptTemplate` creates a structured dialogue that guides the model to generate responses that are both contextually relevant and properly formatted.\n",
    "\n",
    "### Chains\n",
    "\n",
    "Chains in LangChain are modular sequences that connect various componentsâ€”such as prompt templates, language models, and output parsersâ€”into a unified workflow.  They enable developers to construct complex, multi-step processing pipelines where each step transforms or utilizes the output from the previous one.  LCEL (LangChain Chain Expression Language) implements this concept using a concise, pipe operator (|) syntax that clearly defines the data flow between these components.  By leveraging LCEL, you can declaratively compose chains that are both flexible and readable, allowing for rapid prototyping and iterative development of sophisticated AI-powered applications.\n",
    "\n",
    "### Output Parsers\n",
    "\n",
    "The `AIMessage` format might not be the best way for the output of your LLM call to be rendered.  So we can keep adding to the chain by adding another component: `StrOutputParser`.  It converts a language model's output into a clean, plain text string by stripping away any additional metadata or formatting.  So we can see in this example that we just add it modularly to the end of our chain.  There are several other output parsers that you might find helpful.  You can find a listing of them in [this LangChain documentation](https://python.langchain.com/docs/how_to/#output-parsers). \n",
    "\n",
    "In this code block we are creating a simple, three-part chain with our prompt, our LLM, and our output parser.\n",
    "\n",
    "And be sure to check out the system prompt to understand the response you are about to get!  We are not going to tinker too much with prompts just yet.  That will be the subject of the next activity.  Just for now know that you can add some basic instructions here to tell the model what you are looking for.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b88e7b0-db15-4605-b857-c7ba2a1cbe11",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (1697979344.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[21], line 10\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(chain.invoke({\"Create a YAML linter to generate a YAML config with three Python linter rules for a codebase.Each rule must include:rule_name: short, descriptive stringseverity: \"warning\" or \"error\"description: brief explanationThe rules you must generate are:no-hardcoded-credentials â€” flag hardcoded secrets like API keys or DB passwordsvalidate-currency-format â€” ensure currency values are consistently formatteduse-typed-annotations â€” encourage use of Python type hints for clarityUse a custom output parser that:Strips markdown/code fencesParses the YAML with yaml.safe_load()Validates that rules is a list of 3 dicts, each with the required keysYour final output must be valid YAML only â€” no markdown, no extra text.Example:rules:  - rule_name: no-hardcoded-credentials    severity: error    description: Prevent hardcoding of sensitive credentials like API keys.  ...\"}))\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are a helpful assistant.\"\n",
    ")\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])\n",
    "\n",
    "# Chain with prompt and LLM\n",
    "chain = prompt | chat_llm_temp | StrOutputParser()\n",
    "#print(chain.invoke({\"\"}))\n",
    "print(chain.invoke({\"Create a YAML linter to generate a YAML config with three Python linter rules for a codebase.Each rule must include:rule_name: short, descriptive stringseverity: \"warning\" or \"error\"description: brief explanationThe rules you must generate are:no-hardcoded-credentials â€” flag hardcoded secrets like API keys or DB passwordsvalidate-currency-format â€” ensure currency values are consistently formatteduse-typed-annotations â€” encourage use of Python type hints for clarityUse a custom output parser that:Strips markdown/code fencesParses the YAML with yaml.safe_load()Validates that rules is a list of 3 dicts, each with the required keysYour final output must be valid YAML only â€” no markdown, no extra text.Example:rules:  - rule_name: no-hardcoded-credentials    severity: error    description: Prevent hardcoding of sensitive credentials like API keys.  ...\n",
    "\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33edab11-ea29-4ee3-b933-c566abbf4183",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
